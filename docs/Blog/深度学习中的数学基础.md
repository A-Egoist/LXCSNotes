本篇博客不包含深度学习中所有的数学知识，只是我在学习过程中，对不会或者不熟悉的数学知识的记录，因此部分内容和推导我会省略掉。

# 微积分

# 线性代数

# 概率论

**条件概率**
$$
P(A|B)=\frac{P(A,B)}{P(B)}
$$
**乘法公式**
$$
P(A,B)=P(B)\cdot P(A|B)\\s.t.\ P(B)\ >0\\
P(A,B)=P(A)\cdot P(B|A)\\s.t.\ P(A)\ >0\\
$$
**乘法公式推广**
$$
P(A_1,A_2,A_3)=P(A_1)\cdot P(A_2|A_1)\cdot P(A_3|A_1,A_2)\\
\\
P(A_1,\dots,A_n)=P(A_1)\cdot P(A_2|A_1)\cdot P(A_3|A_1,A_2)\cdot P(A_4|A_1,A_2,A_3)\cdots\\s.t.\ P(A_1)>0,\dots ,P(A_1,\dots,A_{n-1})>0
$$
**全概率公式**

知原因推结果
$$
P(A)=\sum_{i=1}^nP(B_i)P(A|B_i)
$$
**Bayes 公式**

知结果推原因
$$
P(B_i|A)=\frac{P(A,B_i)}{P(A)}=\frac{P(B_i)P(A|B_i)}{\sum_{i=1}^nP(B_i)P(A|B_i)},i=1,\dots,n\\s.t.\ P(A|B_i)>0,i=1,\dots,n
$$
**伯努利概型**

定义：试验结果只有两个的试验 E，称为伯努利试验

定理：$n$ 重伯努利概型中，设 $P(A)=p$，$P(\overline{A})=1-p$，则 $n$ 重伯努利概型中 $A$ 发生 $k$ 次的概率为
$$
P_n(k)=C_n^k\cdot{p^k}\cdot{(1-p)^{(n-k)}},k=0,1,\dots,n
$$


**二项分布** $X\sim B(n,p)$

在 $n$ 重伯努利概型中，设 $P(A)=p$，$P(\overline{A})=1-p=q$，$X$ 为 $A$ 发生的次数，则
$$
P(X=k)=C_n^k\cdot{p^k}\cdot{q^{(n-k)}},k=0,1,\dots,n
$$
**泊松分布** $X\sim P(\lambda)$
$$
P(X=k)=\frac{\lambda^ke^{-\lambda}}{k!}\\s.t.\ \lambda>0且为常数,k=0,1,\dots
$$
==注== 若 $X\sim B(n,p)$，$n>10$，$p<0.1$ 时，$P(X=k)\approx \frac{\lambda^ke^{-\lambda}}{k!}$

**均匀分布** $X\sim \mu[a,b]$



**指数分布** $X\sim e(\lambda)$



**正态分布** $X\sim N(\mu,\theta^2)$



**标准正态分布** $X\sim N(0,1)$





# 信息论

### **自信息**

任何事件都会承载着一定的信息量，包括已经发生的事件和未发生的事件，只是它们承载的信息量会有所不同。

如昨天下雨这个已知事件，因为已经发生，既定事实，那么它的信息量就为 $0$。如明天会下雨这个事件，因为未有发生，那么这个事件的信息量就大。

从上面例子可以看出信息量是一个与事件发生概率相关的概念，而且可以得出，事件发生的概率越小，其信息量越大。

定义事件 $X=x$ 的自信息为
$$
I\left(x\right)=-\log_2 P\left(x\right)
$$
### **香农熵(Shannon Entropy)**

当一个事件发生的概率为 $p\left(x\right)$，那么它的信息量是 $-\log{p\left(x\right)}$

如果把这个事件的所有可能性罗列出来，就可以求得该事件信息量的期望

信息量的期望就是熵，所以熵的公式为
$$
H\left(\mathrm{x}\right)=\mathbb{E}_{\mathrm{x}\sim P}\left[I\left(x\right)\right]=-\mathbb{E}_{\mathrm{x}\sim P}\left[\log{P\left(x\right)}\right]
$$
也可以表述为
$$
H\left(\mathrm{x}\right)=-\sum_{i=1}^{n}p\left(x_i\right)\log{p\left(x_i\right)}
$$
对于 0-1 分布问题，熵的计算可以简化为
$$
H\left(\mathrm{x}\right)=-p\left(x\right)\log{\left(p\left(x\right)\right)}-\left(1-p\left(x\right)\right)\log\left(1-p\left(x\right)\right)
$$

### **相对熵(KL 散度)(Kullback-Leibler divergence)**

对于同一个随机变量 $\mathbf{x}$，如果其有两个单独的概率分布 $P(\mathbf{x})$ 和 $Q(\mathbf{x})$，可以使用 KL 散度来衡量这两个分布的差异，KL 散度越小，真实分布与近似分布之间的匹配就越好。
$$
D_{\mathrm{KL}}(P \| Q)=\mathbb{E}_{\mathrm{x} \sim P}\left[\log \frac{P(x)}{Q(x)}\right]=\mathbb{E}_{\mathrm{x} \sim P}[\log P(x)-\log Q(x)]
$$
在机器学习中，$P$ 往往用来表示样本的真实分布，$Q$ 用来表示模型所预测的分布，那么 KL 散度就可以计算两个分布的差异，也就是 Loss
$$
D_{KL}(P\|Q)=\sum_{i=1}^n{P(x_i)\log{\frac{P(x_i)}{Q(x_i)}}}
$$
因为 KL 散度是非负的并且衡量的是两个分布之间的差异，它经常被用作分布之间的某种距离。然而，它并不是真的距离，因为它不是对称的：对于某些 $P$ 和 $Q$，$D_{KL}(P\|Q)\ne D_{KL}(Q\|P)$。这种非对称性意味着选择 $D_{KL}(P\|Q)$ 还是 $D_{KL}(Q\|P)$ 影响很大。

对于 KL 散度的直观解释可以看这篇文章：[初学机器学习：直观解读KL散度的数学概念](https://zhuanlan.zhihu.com/p/37452654)

如何证明 KL 散度大于等于 $0$ (未解决)

### **交叉熵(Cross Entropy)**

两个分布 $P$ 和 $Q$ 之间的交叉熵定义为：
$$
H\left(P,Q\right)=-\mathbb{E}_{x\sim P}\log{Q\left(x\right)}
$$


将 KL 散度公式变形：
$$
\begin{align}
D_{\mathrm{KL}}(P \| Q)&=\mathbb{E}_{\mathrm{x} \sim P}[\log P(x)-\log Q(x)]\\&=\mathbb{E}_{\mathrm{x}\sim{P}}[\log{P(x)}]-\mathbb{E}_{\mathrm{x}\sim{P}}[\log{Q(x)}]\\&=-H(P)-\mathbb{E}_{\mathbf{x}\sim P}[\log Q(x)]\\&=-H(P)+H(P,Q)
\end{align}
$$

所以可得到交叉熵和 KL 散度的关系：
$$
H(P,Q)=H(P)+D_{KL}(P\|Q)
$$
在机器学习中，需要评估 label 和 predicts 之间的差距，使用 KL 散度刚刚好，即 $D_{KL}(\mathbf{y}\|\hat{\mathbf{y}})$。由于 KL 散度中的前一部分 $−H(y)$ 不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用交叉熵做 loss，评估模型。

在信息论中，$\lim_{x\to 0}x\log x=0$

### **JS 散度(Jensen-Shannon divergence)**

**JS 散度**度量了两个概率分布的相似度，基于 KL 散度的变体，解决了 KL 散度非对称的问题。

一般地，JS 散度是对称的，其取值是 $0$ 到 $1$ 之间。定义如下：
$$
JS(P_1\|P_2)=\frac{1}{2}KL(P_1\|\frac{P_1+P_2}{2})+\frac{1}{2}KL(P_2\|\frac{P_1+P_2}{2})
$$

### **Wasserstein 距离**

参考资料：[交叉熵、相对熵（KL散度）、JS散度和Wasserstein距离（推土机距离）](https://zhuanlan.zhihu.com/p/74075915)

KL 散度和 JS 散度的问题

* 如果两个分布 $P,Q$ 离得很远，完全没有重叠的时候，那么 KL 散度值是没有意义的，而 JS 散度值是一个常数。这在学习算法中是比较致命的，这就意味着这一点的梯度为 $0$，梯度消失了。

**Wasserstein距离** 度量两个概率分布之间的距离，定义如下
$$
W(P_1,P_2)=\inf_{\gamma\sim\prod(P_1,P_2)}\mathbb{E}_{(x,y)\sim\gamma}[\left\|x-y\right\|]
$$


# 补充知识



### **Jensen 不等式**

参考资料：[Jensen不等式初步理解及证明](https://zhuanlan.zhihu.com/p/39315786)

