# Smale 机器学习常识 2023 第三讲
> 主讲人: [闵帆](https://blog.csdn.net/minfanphd)
>
> 时间: 2023.06.21 14:30-15:30
>
> 文字稿: [机器学习常识 6: kNN](https://blog.csdn.net/minfanphd/article/details/130918679)、[机器学习常识 7: 决策树](https://blog.csdn.net/minfanphd/article/details/130930192)、

## 常识 6: KNN

摘要: 具有讽刺意味的是: 机器学习最基本的算法居然是不学习, 也称为惰性学习(lazy learning). KNN(K-Nearest Neighbor)通过计算样本间的距离(相似度)来确定待预测样本应与哪些训练样本的标签保持一致.

### 基本思想

为了应对一场考试, 我们可以不学习, 而在考场上去翻书. 是的, 在解决实际问题时, 完全可以是开卷考试.
继续使用前面提到的例子. 上午来了 60 个就诊者, 根据他们的各项检测指标(即数据), 主治医生给出了诊断结论(如是否患病, 以及患哪种病). 实习医生只是用小本本把带标签数据记录下来, 并没往心里去. 下午来了 1 个就诊者 A, 实习医生将他的检测指标与上午 60 个就诊者的数据逐一比对, 并找出 $k=3$ 个最相似的. 根据这 $k$ 个就诊者的情况, 就可以对 A 的患病情况进行预测(例如, 他们都没患病, 就预测 A 也没病).
这种方式极度简单, 但揭示了人类认识世界的一个本质而有效的思想: 根据相似性进行预测.
人们常说"见多识广", 本质上就是大脑内存储的样本多, 就可以对新样本进行准确的判断. 大医院的医生更靠谱, 一些小医院所说的疑难杂症, 对他们而言可能是天天见的病例. 因此, 在学习更多的算法之后, 仍然需要记住一句话: 永远不要小瞧 KNN. 你很难找到一种算法在随机选择的 100 个数据集上完美地打败它.
前段时间热炒的大数据概念, 其中一个思想就是: 如果训练数据的量足够大, 那么我们只需要用很简单方法(如 KNN)即可. 假设一个实习生见到的不是 60 个样本, 而是 1 亿个样本, 那么他的判断应该可以达到惊人的准确性. 西瓜书上也说了, 理论上可以证明, 当样本量足够大时, KNN 的误差不超过贝叶斯误差(即理论最大误差)的 2 倍.

### 参数设置

$k$ 是一个需要调整的参数. 如果是二分类问题, 一般将其设置为奇数, 方便投票.

### 距离度量(distance measure)

欧氏距离

曼哈顿距离

### 主要缺点

慢.

试想别人学习到知识, 考试的时候刷刷地做题, 而你要一页页地翻书, 速度就不是一个量级的了.
为了缓解这个问题, 可以先把 10,000 条训练数据聚为 50 个簇, 每簇约 200 个样本 (聚类问题), 并获得这些簇中心点. 进行比对的时候, 先确定与哪个簇中心最近, 然后再在这个簇中心找邻居. 这样就可以把 10,000 次对比降为约 50 + 200 次.

### 归一化(normalization)

为消除不同指标取值范围的影响, 可使用归一化, 即所有指标均取 $[0, 1]$ 区间的值.

### 度量学习(metric learning)

归一化还是会引入新的问题: 认为所有的指标同等重要. 在现实世界中, 应该为每个指标赋予一定的权重 $w$, 并使得该指标的取值范围为 $[0,w]$. 相应的权值可以从数据中学习到. 这就是度量学习的初衷.

### 常见误区

*   小瞧 KNN, 觉得它过于简单.
*   没有认识到 KNN 正是人类认识物体的基本方式.
*   不知道 KNN 可以做回归(把邻居的实数型标签值取平均即可).

### 延伸阅读

KNN 的 Java 代码: [日撸 Java 三百行（51-60天，kNN 与 NB）](https://blog.csdn.net/minfanphd/article/details/116975957)

## 常识 7: 决策树

摘要: 决策树是一种与人类思维一致, 可解释的模型.

### 决策树的结构



### 决策树的优势

-   直观, 易于理解, 易于传授: 学生会迅速掌握这棵树.
-   相比于决策规则集合, 决策树的优点是没有死角: 任何一种情况都被覆盖.
-   与前面的 KNN 相比, 它是一个真正的模型(model), 预测阶段脱离了训练数据.
-   对于机器而言, 使用决策树进行预测非常迅速, 任何新的实例, 都对应于从树根走到某个叶节点的一条路径. 这种路径的长度通常不超过 10.

### 决策树的原理

人为可以构建决策树, 这就是专家知识, 但它不属于我们重点讨论的内容.
从数据中构建出决策树, 才是机器学习的内容.
决策树的构建过程, 实际上是一个不完全归纳(特殊到一般)的过程. 为学习到图 1 所示的决策树, 只用了 $14$ 个样本. 但这棵决策树所覆盖的可能情况, 远远超过了 $14$. outlook 有 $3$ 种情况, humidity 有 $100$ 种情况, rain 有 $2$ 种情况, windy 有 $2$ 种情况, 所以总共是 $3 \times 100 \times 2 \times 2 = 1200$ 种情况.
决策树构建的原则是: 越小越好, 即节点树越少越好. 这是基于奥克姆剃刀(Occam’s razor)原理.

### 决策树的构建方法

-   穷举法. 由于数据量比较大, 一般不使用这种方法.
-   启发式方法. 如基于信息熵、基于基尼指数. ID3 适用于枚举型数据, 使用了信息熵(条件信息熵之差称为信息增益). 对于实数型数据, 则使用 C4.5. 在绝大多数情况下, ID3 可以获得最小的决策树. 但你也可以构造出反例. 在 2000 年前, 决策树火得一蹋糊涂.
-   剪枝. 如果一棵决策树使用一张 A4 纸都画不下, 就失去了泛化能力. 这时候需要剪枝. 例如, 在一个节点处, 有 100 个正样本和 1 个负样本, 虽然可以增加一个属性将它们分开, 但最好不要增加这个属性, 这样节点至少节约了一个.
-   常规的决策树, 其分割面都垂直于相应特征的坐标轴. 有时候想同时考虑多个属性组合而成(温度 + 湿度)的新属性, 可以使用 Oblique decision tree (斜决策树).

### 常见误区

-   没有认识到师父可以将他大脑里的决策树当成知识, 直接教给徒弟.
-   没有认识到现成的决策树, 其实也是前人从数据中学习而得的知识.
-   没有把决策树看成一系列“具有良好组织”的规则.
-   一开始就陷入信息增益的概念, 而没想到决策树其实是一大类方法, 还可以用其它启发式信息. 只是 ID3 最为成功而已.

