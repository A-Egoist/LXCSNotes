# 感知机模型

## 感知机模型

感知机是根据输入实例的特征向量 $x$ 对其进行二类分类的线性分类模型：
$$
f(x)=sign(w\cdot x+b)
$$
感知机模型对应于输入空间（特征空间）中的分离超平面$w \cdot x+b=0$。

## 感知机学习策略

极小化损失函数：
$$
\min _{w, b} L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
$$
损失函数对应于误分类点到分离超平面的总距离。

## 感知机学习算法

### 原始形式

感知机学习算法是采用误分类驱动的，具体采用随机梯度下降法(SGD)。

其中损失函数 $L(w,b)$ 的梯度由：
$$
\nabla_wL(w,b)=-\sum_{x_i\in M}y_ix_i\\
\nabla_bL(w,b)=-\sum_{x_i\in M}y_i
$$
给出。

随机选取一个误分类点 $(x_i,y_i)$，对 $w,b$ 进行更新：
$$
w\leftarrow w+\eta y_ix_i\\
b\leftarrow b+\eta y_i
$$
其中 $\eta$ 表示学习率(learning rate)。

注：感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同。

### 对偶形式

> 动机：对偶形式的目的是降低每次迭代的运算量，但是并不是在任何情况下都能降低运算量，而是在特征空间的维度远大于数据集大小时才起作用。
>
> 设特征空间是$R^n$，样本数量为$N$，$n\gg N$。对于原始形式，时间复杂度为$\Theta(n)$，而对于对偶形式，时间复杂度为$\Theta(N)$。所以当特征数量远大于训练样本数量的时候，应该采用对偶形式训练，从而达到加速的效果。
>
> [如何理解感知机学习算法的对偶形式？](https://www.zhihu.com/question/26526858/answer/253579695)

对于一个误分类点 $(x_i,y_i)$ 通过：
$$
w\leftarrow w+\eta y_ix_i\\
b\leftarrow b+\eta y_i
$$
逐步修改 $w,b$，设修改 $n$ 次，则 $w,b$ 关于$(x_i,y_i)$的增量分别是 $\alpha_iy_ix_i$ 和 $\alpha_iy_i$，这里 $\alpha_i=n_i\eta_i$，$n_i$ 是点$(x_i,y_i)$被误分类的次数。这样，从学习过程不难看出，最后学习到的 $w,b$ 可以分别表示为：
$$
w=\sum_{i=1}^{N}\alpha_iy_ix_i\\b=\sum_{i=1}^{N}\alpha_iy_i
$$
实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类。换句话说，这样的实例对学习结果影响最大。

## 参考资料

《统计学习方法》