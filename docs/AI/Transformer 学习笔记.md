学习路径: MLP$\to$RNN$\to$seq2seq/encoder and decoder$\to$attention$\to$self-attention$\to$transformer

## Attention

Queries、Keys、Values

## Self-attention

## Transformer

## 参考资料

[1] [注意力机制的本质|Self-Attention|Transformer|QKV矩阵](https://www.bilibili.com/video/BV1dt4y1J7ov/?spm_id_from=333.1245.0.0&vd_source=f4cc25a44af6631d6f4db023b3bb88e4)

[2] [self-Attention｜自注意力机制 ｜位置编码 ｜ 理论 + 代码](https://www.bilibili.com/video/BV1qo4y1F7Ep/?spm_id_from=333.1245.0.0&vd_source=f4cc25a44af6631d6f4db023b3bb88e4)

[3] [注意力机制](https://zh.d2l.ai/chapter_attention-mechanisms/index.html)

[4] [64 注意力机制【动手学深度学习v2】](https://www.bilibili.com/video/BV1264y1i7R1/?spm_id_from=444.41.top_right_bar_window_view_later.content.click&vd_source=f4cc25a44af6631d6f4db023b3bb88e4)

